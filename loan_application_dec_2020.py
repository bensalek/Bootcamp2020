# -*- coding: utf-8 -*-
"""Loan Application- Dec 2020.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BbH7x8cGC8Lq3QZyEilXAQDhCXTzjtHw

**Import Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Genral Libraries
import pandas as pd
import numpy as np
# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
# Comment this if the data visualisations doesn't work on your side
# %matplotlib inline
# To ignore warnings in the notebook
import warnings
warnings.filterwarnings("ignore")

# to display up to 500 rows in the output of the jupyter notebook cell
pd.set_option('display.max_rows', 500)

#ML Libraries

"""Import Dataset"""

#Importing Training Data
train =  pd.read_csv("https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_train.csv" )
train.shape

#Importing Test Data
test = test_data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_test.csv')
test.shape

"""### **Basic Data Preprocessing**"""

train.head()

test.head()

#Numeric Columns
train.describe()

#Numeric Columns
test.describe()

#Categorical Columns
train.describe(include='object')

#Categorical Columns
test.describe(include='object')

train.info()
# code for transform data type into categorical variable
#train['Survived'] = train['Survived'].astype(str)

test.info()

del train['Unnamed: 0']
del train['Loan_ID']

# getting all the numerical columns
num_cols = train.select_dtypes(include=np.number)

# getting all the categorical columns
cat_cols = train.select_dtypes(include = 'object')

"""## Data Visualization"""

# get the correlation from the dataframe, then plug it to heatmap function and show it
corr = train.corr()


# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()

#Single Column Count   Question::: How to plot multiple plots!
sns.countplot(x='Credit_History', data=train, hue='Loan_Status')

"""**Seperating numeric coulns in df_num**"""

df_num = train.select_dtypes(include = ['float64', 'int64'])
df_num.hist(figsize=(16, 20), bins=30, xlabelsize=8, ylabelsize=8);

#Pairplot for regression problems   y_vars=['SalePrice'] should be replace with the target variable

for i in range(0, len(df_num.columns), 5):
    sns.pairplot(data=df_num,
                x_vars=df_num.columns[i:i+5],
                y_vars=['Loan_Status'])

"""### **Handling Missing Values**"""

# detect columns with missing values in the train data
# isnull().sum() will sum the number of missing values by columns
train_count_of_missval_by_col = (train.isnull().sum())

# below code will display only the columns with missing values (in absolute number of rows)
print('Missing values \n\n',train_count_of_missval_by_col[train_count_of_missval_by_col > 0])

# below code will display only the columns with missing values (in percentage of missing values to the total rows)
print('Missing value percentage\n\n',(train_count_of_missval_by_col[train_count_of_missval_by_col > 0]/train.shape[0])*100)

# detect columns with missing values in the test data
# isnull().sum() will sum the number of missing values by columns
test_count_of_missval_by_col = (test.isnull().sum())
test_count_of_missval_by_col

# below code will display only the columns with missing values (in absolute number of rows)
print('Missing values \n\n',test_count_of_missval_by_col[test_count_of_missval_by_col > 0])

# below code will display only the columns with missing values (in percentage of missing values to the total rows)
print('Missing value percentage\n\n',(test_count_of_missval_by_col[test_count_of_missval_by_col > 0]/test.shape[0])*100)

# Missing value treatment
# missing values - too many missing values - dropping entire column

# missing values in numeric column many not be NaN or blank. It could be zero as well
#train[train['Fare'] == 0].shape

# droping a column
#Code df.drop('column_name', axis=1, inplace=True)

#train['Credit_History'] = train['Credit_History'].astype(str)

#Filling Missing Values ( Another method is sklearn impute)
#fillna() can “fill in” missing values in a couple of ways:

# For example, we can use fillna() to replace missing values with the mean value for each column, as follows:
train['LoanAmount'].fillna(train['LoanAmount'].mean(), inplace=True)
train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mean(), inplace=True)
train['Gender'].fillna(train['Gender'].mode().iloc[0], inplace=True)
train['Married'].fillna(train['Married'].mode().iloc[0], inplace=True)
train['Dependents'].fillna(train['Dependents'].mode().iloc[0], inplace=True)
train['Self_Employed'].fillna(train['Self_Employed'].mode().iloc[0], inplace=True)
train['Credit_History'].fillna(train['Credit_History'].mode().iloc[0], inplace=True)



test['LoanAmount'].fillna(test['LoanAmount'].mean(), inplace=True)
test['Loan_Amount_Term'].fillna(test['Loan_Amount_Term'].mean(), inplace=True)
test['Gender'].fillna(test['Gender'].mode().iloc[0], inplace=True)
test['Married'].fillna(test['Married'].mode().iloc[0], inplace=True)
test['Dependents'].fillna(test['Dependents'].mode().iloc[0], inplace=True)
test['Self_Employed'].fillna(test['Self_Employed'].mode().iloc[0], inplace=True)
test['Credit_History'].fillna(test['Credit_History'].mode().iloc[0], inplace=True)

# Replace NA with a scalar value or a string:
# df.fillna(0), df.fillna("missing")
#  Fill the gaps forward or backward: Using the filling arguments, we can propagate non-NA values forward or backward:

#  df.fillna(method='pad') , df.fillna(method='ffill') fill values forward

# df.fillna(method='backfill') bfill / backfill fills values backward
# fills the missing values with maximum occuring element in the column
# code fraud_data[cat_cols] = fraud_data[cat_cols].fillna(fraud_data[cat_cols].mode().iloc[0])

"""### **Splitting X and Y**"""

# Separate input features and output feature
# input features
X = train.drop(columns = ['Loan_Status'])       

# output feature
Y = train.Loan_Status

from sklearn.preprocessing import LabelEncoder

X[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Property_Area']]=X[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Property_Area']].apply(LabelEncoder().fit_transform)

X.head()

train.head()

cat_cols.columns

X.info()

"""**Splitting the data**

Split into train and validation set

Train – what we use to train the 

Validation – what we use to evaluate the model

Test – data that is unexposed to the model
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)

from xgboost import XGBClassifier

from sklearn.linear_model import LogisticRegression

XGmodel = XGBClassifier()
XGmodel.fit(X_train, y_train)

# finding our predicted values
#y_pred = lr_baseline_model.predict(X_test)

# Calculating the accuracy and F1 Score by comparing the actual and predicted values
#ac = accuracy_score(y_test, y_pred)
#f_score = f1_score(y_test ,y_pred)

#print("Baseline Model Accuracy:", ac)
#print("Baseline Model F1 Score:", f_score)

test[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Property_Area']]=test[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Property_Area']].apply(LabelEncoder().fit_transform)

del test['Loan_ID']

pred = XGmodel.predict(test)

n=pd.DataFrame(pred)

from google.colab import  drive

drive.mount('/drive')

n.to_csv('/drive/My Drive/Colab Notebooks/datathon3.csv')

import pickle

pickle.dump(XGmodel, open('Loan_model.pkl','wb'))